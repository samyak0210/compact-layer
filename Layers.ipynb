{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compact Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompactLayer1(layers, kernel_sizes, strides=None, paddings=None, biases=None, Relus=None,\n",
    "                  max_pool_kernel_sizes=None, max_pool_strides=None, max_pool_paddings=None,\n",
    "                  max_pool_dilations=None):\n",
    "    '''\n",
    "    Args:\n",
    "        layers       (list) : List of tuples consisting of number of in channels and out channels\n",
    "        kernel_sizes (list) : List of tuples consisting of kernel sizes\n",
    "        strides      (list) : List of tuples consisting of stride length. Can be -1 or tuple\n",
    "                              Default is None  \n",
    "        paddings     (list) : List of tuples consisting of padding dim. Can be -1 or tuple\n",
    "                              Default is None\n",
    "        biases       (list) : List of boolean value stating whether bias is required or not.\n",
    "                              Default is None\n",
    "        Relus        (list) : List of boolean value stating whether ReLU \n",
    "                              layer is required or not.True or False.\n",
    "                              Default is None\n",
    "        \n",
    "        max_pool_kernel_sizes (list) : List of integers or tuples denoting the kernel size\n",
    "                                       for MaxPool2D layer. Default is None.\n",
    "        \n",
    "        max_pool_strides      (list) : List of integers or tuples denoting the stride for\n",
    "                                       MaxPool2D layer. Can be -1 also to set to default.\n",
    "        \n",
    "        max_pool_paddings     (list) : List of integers denoting the padding for MaxPool2D \n",
    "                                       layer. Can be -1 also to set to default.\n",
    "        \n",
    "        max_pool_dilations    (list) : List of integers denoting the dilations for MaxPool2D\n",
    "                                       layer. Can be -1 also to set to default.\n",
    "     \n",
    "    Returns:\n",
    "        network  (nn.Sequential) : Compact layer consisting of sequence of Convolution layer, ReLUs \n",
    "                                   and MaxPool2D layers.\n",
    "        \n",
    "    Example:\n",
    "    >>> layers = [(3,64),(64,64)]\n",
    "    >>> kernel_sizes = [(3,3),(5,5)]\n",
    "    >>> strides = [(1,1),None]\n",
    "    >>> paddings = [None,(1,1)]\n",
    "    >>> biases = [None, True]\n",
    "    >>> Relus = [True,False]\n",
    "    >>> max_pool_kernel_sizes = [2, None]\n",
    "    >>> net = CompactLayer1(layers, kernel_sizes, strides, paddings, biases, Relus, max_pool_kernel_sizes)\n",
    "    >>> net\n",
    "    Sequential(\n",
    "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
    "      (1): ReLU()\n",
    "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "      (3): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    model = []\n",
    "    \n",
    "    if strides is None:\n",
    "        strides = [-1 for _ in range(len(layers))]\n",
    "    \n",
    "    if paddings is None:\n",
    "        paddings = [-1 for _ in range(len(layers))]\n",
    "    \n",
    "    if biases is None:\n",
    "        biases = [-1 for _ in range(len(layers))]\n",
    "    \n",
    "    if Relus is None:\n",
    "        Relus = [False for _ in range(len(layers))]\n",
    "    \n",
    "    if max_pool_kernel_sizes is None:\n",
    "        max_pool_kernel_sizes = [None for _ in range(len(layers))]\n",
    "        \n",
    "    if max_pool_strides is None:\n",
    "        max_pool_strides = [-1 for _ in range(len(layers))]\n",
    "    \n",
    "    if max_pool_paddings is None:\n",
    "        max_pool_paddings = [-1 for _ in range(len(layers))]\n",
    "    \n",
    "    if max_pool_dilations is None:\n",
    "        max_pool_dilations = [-1 for _ in range(len(layers))]\n",
    "    \n",
    "    # The length of all lists must be same\n",
    "    assert len(layers) == len(kernel_sizes) == len(strides) ==\\\n",
    "            len(paddings) == len(biases) == len(Relus) ==\\\n",
    "            len(max_pool_kernel_sizes) == len(max_pool_strides) ==\\\n",
    "            len(max_pool_paddings) == len(max_pool_dilations)\n",
    "    \n",
    "    for i in range(len(layers)):\n",
    "        \n",
    "        stride = strides[i]\n",
    "        padding = paddings[i]\n",
    "        bias = biases[i]\n",
    "        \n",
    "        max_pool_stride = max_pool_strides[i]\n",
    "        max_pool_padding = max_pool_paddings[i]\n",
    "        max_pool_dilation = max_pool_dilations[i]\n",
    "            \n",
    "        \n",
    "        if stride == -1:\n",
    "            # If the stride is -1 the default value 1 is set\n",
    "            stride = 1\n",
    "            \n",
    "        if padding == -1:\n",
    "            # If the padding is -1 the default value 0 is set\n",
    "            padding = 0\n",
    "        \n",
    "        if bias == -1:\n",
    "            # If bias is -1 it is set True\n",
    "            bias = True\n",
    "        \n",
    "        if max_pool_stride == -1:\n",
    "            # If max_pool_stride is -1 the default value None is set \n",
    "            max_pool_stride = None\n",
    "        \n",
    "        if max_pool_padding == -1:\n",
    "            # If max_pool_padding is -1 the default value 0 is set \n",
    "            max_pool_padding = 0\n",
    "        \n",
    "        if max_pool_dilation == -1:\n",
    "            # If max_pool_dilation is -1 the default value 1 is set \n",
    "            max_pool_dilation = 1\n",
    "        \n",
    "        \n",
    "        conv2d = nn.Conv2d(layers[i][0], layers[i][1], kernel_size=kernel_sizes[i], stride=stride, \n",
    "                           padding=padding, bias=bias)\n",
    "        \n",
    "        model.append(conv2d)\n",
    "        \n",
    "        if Relus[i]:\n",
    "            model.append(nn.ReLU())\n",
    "                \n",
    "        if max_pool_kernel_sizes[i] is not None:\n",
    "            max_pool = nn.MaxPool2d(max_pool_kernel_sizes[i], stride=max_pool_stride,\n",
    "                                    padding=max_pool_padding, dilation=max_pool_dilation)\n",
    "            model.append(max_pool)\n",
    "    \n",
    "    network = nn.Sequential(*model)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [(3,64), (64,64)]\n",
    "kernel_sizes = [(3,3), (5,5)]\n",
    "strides = [(1,1), -1]\n",
    "paddings = [-1, (1,1)]\n",
    "biases = [-1, True]\n",
    "Relus = [True, False]\n",
    "max_pool_kernel_sizes = [2, None]\n",
    "net = CompactLayer1(layers, kernel_sizes, strides, paddings, biases, Relus, max_pool_kernel_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 24, 24])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The convolutional layers change the number of channels from 3 to 64\n",
    "channel = torch.ones((1, 3, 28, 28))\n",
    "net(channel).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compact Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompactLayer2(features, biases=None, Relus=None, dropouts=None):\n",
    "    '''\n",
    "    Args:\n",
    "        features     (list) : List of tuples consisting of in features and \n",
    "                              out features for the Linear layer\n",
    "        biases       (list) : List of boolean value stating whether bias is required or not.\n",
    "                              Default is None.\n",
    "        Relus        (list) : List of boolean value stating whether ReLU \n",
    "                              layer is required or not.True or False.\n",
    "                              Default is None.\n",
    "        dropouts     (list) : List of tuples consisting whether dropout \n",
    "                              is required or not and the dropout parameter p.\n",
    "                              Default is None.\n",
    "    Returns:\n",
    "        network  (nn.Sequential) : Compact layer consisting of sequence of \n",
    "                                   Linear layers, ReLUs and Dropout Layers.\n",
    "    \n",
    "    Example:\n",
    "    >>> features = [(3,3),(3,5)]\n",
    "    >>> biases = [True, None]\n",
    "    >>> Relus = [True, True]\n",
    "    >>> dropouts = [(True, None), (False, 0.5)]\n",
    "    >>> network = CompactLayer2(features, biases, Relus, dropouts)\n",
    "    >>> network\n",
    "    Sequential(\n",
    "      (0): Linear(in_features=3, out_features=3, bias=True)\n",
    "      (1): ReLU()\n",
    "      (2): Dropout(p=0.5)\n",
    "      (3): Linear(in_features=3, out_features=5, bias=True)\n",
    "      (4): ReLU()\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    model = []\n",
    "    \n",
    "    if Relus is None:\n",
    "        Relus = [False for _ in range(len(layers))]\n",
    "    \n",
    "    if biases is None:\n",
    "        biases = [-1 for _ in range(len(layers))]\n",
    "        \n",
    "    if dropouts is None:\n",
    "        dropouts = [-1 for _ in range(len(layers))]\n",
    "    \n",
    "    # The length of all lists must be same\n",
    "    assert len(features) == len(dropouts) == len(biases) == len(Relus)\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        \n",
    "        in_features = features[i][0]\n",
    "        out_features = features[i][1]\n",
    "        is_drop, dropout = dropouts[i]\n",
    "        bias = biases[i]\n",
    "        \n",
    "        if dropout == -1:\n",
    "            # If the dropout probablity is None the default value 1 is set\n",
    "            dropout = 0.5\n",
    "        if bias == -1:\n",
    "            # If bias is None it is set True\n",
    "            bias = True\n",
    "        \n",
    "        linear_layer = nn.Linear(in_features=in_features, out_features=out_features, bias=bias)\n",
    "        \n",
    "        model.append(linear_layer)\n",
    "        \n",
    "        if Relus[i]:\n",
    "            model.append(nn.ReLU())\n",
    "        \n",
    "        if is_drop:\n",
    "            model.append(nn.Dropout(p=dropout))\n",
    "            \n",
    "    network = nn.Sequential(*model)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [(3,3),(3,5)]\n",
    "biases = [True, -1]\n",
    "Relus = [True, True]\n",
    "dropouts = [(True, -1), (False, -1)]\n",
    "network = CompactLayer2(features, biases, Relus, dropouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.5)\n",
       "  (3): Linear(in_features=3, out_features=5, bias=True)\n",
       "  (4): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.5209, 0.0000]], grad_fn=<ThresholdBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear is a Float Tensor of size 3 \n",
    "# The architecture is Linear (3, 3) -> ReLU -> Dropout(p=0.5) -> Linear (3, 5) -> ReLU\n",
    "# Output will be Float Tensor of size 5\n",
    "linear = torch.FloatTensor([[1, 2, 3]])\n",
    "network(linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConnectLayers(*args):\n",
    "    '''\n",
    "    Args:\n",
    "        *args    (list) : Variable number of arguments of networks to convert\n",
    "                          into a single network.\n",
    "    Returns:\n",
    "        network  (nn.Sequential) : Connects all the networks.\n",
    "    \n",
    "    Example:\n",
    "    >>> network1\n",
    "    Sequential(\n",
    "      (0): Linear(in_features=3, out_features=3, bias=True)\n",
    "      (1): ReLU()\n",
    "      (2): Dropout(p=0.5)\n",
    "      (3): Linear(in_features=3, out_features=5, bias=True)\n",
    "      (4): ReLU()\n",
    "    )\n",
    "    >>> network2\n",
    "    Sequential(\n",
    "      (0): Linear(in_features=5, out_features=7, bias=True)\n",
    "      (1): ReLU()\n",
    "      (2): Dropout(p=0.5)\n",
    "      (3): Linear(in_features=7, out_features=9, bias=True)\n",
    "      (4): ReLU()\n",
    "    )\n",
    "    >>> net = connectLayers(network1, network2)\n",
    "    >>> net\n",
    "    Sequential(\n",
    "      (0): Linear(in_features=3, out_features=3, bias=True)\n",
    "      (1): ReLU()\n",
    "      (2): Dropout(p=0.5)\n",
    "      (3): Linear(in_features=3, out_features=5, bias=True)\n",
    "      (4): ReLU()\n",
    "      (5): Linear(in_features=5, out_features=7, bias=True)\n",
    "      (6): ReLU()\n",
    "      (7): Dropout(p=0.5)\n",
    "      (8): Linear(in_features=7, out_features=9, bias=True)\n",
    "      (9): ReLU()\n",
    "    )\n",
    "    '''\n",
    "    finalNet = []\n",
    "    for arg in args:\n",
    "        finalNet.extend(list(arg.children()))\n",
    "    finalNet = nn.Sequential(*finalNet)\n",
    "    return finalNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.5)\n",
       "  (3): Linear(in_features=3, out_features=5, bias=True)\n",
       "  (4): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [(3,3),(3,5)]\n",
    "biases = [True, None]\n",
    "Relus = [True, True]\n",
    "dropouts = [(True, None), (False, 0.5)]\n",
    "network1 = CompactLayer2(features, biases, Relus, dropouts)\n",
    "network1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=5, out_features=7, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.5)\n",
       "  (3): Linear(in_features=7, out_features=9, bias=True)\n",
       "  (4): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [(5,7),(7,9)]\n",
    "biases = [True, None]\n",
    "Relus = [True, True]\n",
    "dropouts = [(True, None), (False, 0.5)]\n",
    "network2 = CompactLayer2(features, biases, Relus, dropouts)\n",
    "network2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.5)\n",
       "  (3): Linear(in_features=3, out_features=5, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=5, out_features=7, bias=True)\n",
       "  (6): ReLU()\n",
       "  (7): Dropout(p=0.5)\n",
       "  (8): Linear(in_features=7, out_features=9, bias=True)\n",
       "  (9): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = connectLayers(network1, network2)\n",
    "net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
