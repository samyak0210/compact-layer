{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compact Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompactLayer1(layers, kernel_sizes, strides, paddings, biases, Relus):\n",
    "    '''\n",
    "    Args:\n",
    "        layers       (list) : List of tuples consisting of number of in channels and out channels\n",
    "        kernel_sizes (list) : List of tuples consisting of kernel sizes\n",
    "        strides      (list) : List of tuples consisting of stride length. Can be None or tuple\n",
    "        paddings     (list) : List of tuples consisting of padding dim. Can be None or tuple\n",
    "        biases       (list) : List of boolean value stating whether bias is required or not.\n",
    "        Relus        (list) : List of boolean value stating whether ReLU \n",
    "                              layer is required or not.True or False\n",
    "    \n",
    "    Returns:\n",
    "        network  (nn.Sequential) : Compact layer consisting of sequence of Convolution layer and ReLUs.\n",
    "        \n",
    "    Example:\n",
    "    >>> layers = [(3,64),(64,64)]\n",
    "    >>> kernel_sizes = [(3,3),(5,5)]\n",
    "    >>> strides = [(1,1),None]\n",
    "    >>> paddings = [None,(1,1)]\n",
    "    >>> biases = [None, True]\n",
    "    >>> Relus = [True,False]\n",
    "    >>> net = CompactLayer1(layers, kernel_sizes, strides, paddings, biases, Relus)\n",
    "    >>> net\n",
    "    Sequential(\n",
    "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
    "      (1): ReLU()\n",
    "      (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
    "    )\n",
    "\n",
    "    '''\n",
    "    \n",
    "    model = []\n",
    "    \n",
    "    # The length of all lists must be same\n",
    "    assert len(layers) == len(kernel_sizes) == len(strides) ==\\\n",
    "            len(paddings) == len(biases) == len(Relus)\n",
    "    \n",
    "    for i in range(len(layers)):\n",
    "        \n",
    "        stride = strides[i]\n",
    "        padding = paddings[i]\n",
    "        bias = biases[i]\n",
    "        \n",
    "        if stride is None:\n",
    "            # If the stride is None the default value 1 is set\n",
    "            stride = 1\n",
    "        if padding is None:\n",
    "            # If the padding is None the default value 0 is set\n",
    "            padding = 0\n",
    "        if bias is None:\n",
    "            # If bias is None it is set True\n",
    "            bias = True\n",
    "        \n",
    "        conv2d = nn.Conv2d(layers[i][0], layers[i][1], kernel_size=kernel_sizes[i], stride=stride, \n",
    "                           padding=padding, bias=bias)\n",
    "        \n",
    "        model.append(conv2d)\n",
    "        \n",
    "        if Relus[i]:\n",
    "            model.append(nn.ReLU())\n",
    "    \n",
    "    network = nn.Sequential(*model)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [(3,64), (64,64)]\n",
    "kernel_sizes = [(3,3), (5,5)]\n",
    "strides = [(1,1), None]\n",
    "paddings = [None, (1,1)]\n",
    "biases = [None, True]\n",
    "Relus = [True, False]\n",
    "net = CompactLayer1(layers, kernel_sizes, strides, paddings, biases, Relus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 24, 24])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The convolutional layers change the number of channels from 3 to 64\n",
    "channel = torch.ones((1, 3, 28, 28))\n",
    "net(channel).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compact Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompactLayer2(features, biases, Relus, dropouts):\n",
    "    '''\n",
    "    Args:\n",
    "        features     (list) : List of tuples consisting of in features and \n",
    "                              out features for the Linear layer\n",
    "        biases       (list) : List of boolean value stating whether bias is required or not.\n",
    "        Relus        (list) : List of boolean value stating whether ReLU \n",
    "                              layer is required or not.True or False\n",
    "        dropouts     (list) : List of tuples consisting whether dropout \n",
    "                              is required or not and the dropout parameter p.\n",
    "    \n",
    "    Returns:\n",
    "        network  (nn.Sequential) : Compact layer consisting of sequence of \n",
    "                                   Linear layers, ReLUs and Dropout Layers.\n",
    "    \n",
    "    Example:\n",
    "    >>> features = [(3,3),(3,5)]\n",
    "    >>> biases = [True, None]\n",
    "    >>> Relus = [True, True]\n",
    "    >>> dropouts = [(True, None), (False, 0.5)]\n",
    "    >>> network = CompactLayer2(features, biases, Relus, dropouts)\n",
    "    >>> network\n",
    "    Sequential(\n",
    "      (0): Linear(in_features=3, out_features=3, bias=True)\n",
    "      (1): ReLU()\n",
    "      (2): Dropout(p=0.5)\n",
    "      (3): Linear(in_features=3, out_features=5, bias=True)\n",
    "      (4): ReLU()\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    model = []\n",
    "    \n",
    "    # The length of all lists must be same\n",
    "    assert len(features) == len(dropouts) == len(biases) == len(Relus)\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        \n",
    "        in_features = features[i][0]\n",
    "        out_features = features[i][1]\n",
    "        is_drop, dropout = dropouts[i]\n",
    "        bias = biases[i]\n",
    "        \n",
    "        if dropout is None:\n",
    "            # If the dropout probablity is None the default value 1 is set\n",
    "            dropout = 0.5\n",
    "        if bias is None:\n",
    "            # If bias is None it is set True\n",
    "            bias = True\n",
    "        \n",
    "        linear_layer = nn.Linear(in_features=in_features, out_features=out_features, bias=bias)\n",
    "        \n",
    "        model.append(linear_layer)\n",
    "        \n",
    "        if Relus[i]:\n",
    "            model.append(nn.ReLU())\n",
    "        \n",
    "        if is_drop:\n",
    "            model.append(nn.Dropout(p=dropout))\n",
    "            \n",
    "    network = nn.Sequential(*model)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [(3,3),(3,5)]\n",
    "biases = [True, None]\n",
    "Relus = [True, True]\n",
    "dropouts = [(True, None), (False, 0.5)]\n",
    "network = CompactLayer2(features, biases, Relus, dropouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.5)\n",
       "  (3): Linear(in_features=3, out_features=5, bias=True)\n",
       "  (4): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear is a Float Tensor of size 3 \n",
    "# The architecture is Linear (3, 3) -> ReLU -> Dropout(p=0.5) -> Linear (3, 5) -> ReLU\n",
    "# Output will be Float Tensor of size 5\n",
    "linear = torch.FloatTensor([[1, 2, 3]])\n",
    "network(linear)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
